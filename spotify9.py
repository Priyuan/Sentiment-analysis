# -*- coding: utf-8 -*-
"""Spotify9.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PxTMiDpjSI3G6s8VbUqIct80PxqHJooy
"""

!pip uninstall -y transformers
!pip install transformers --upgrade --quiet

!pip install pandas numpy sklearn transformers torch datasets
!pip install sentence-transformers
!pip install scikit-learn
!pip install pandas numpy sklearn transformers torch datasets
!pip install sentence-transformers
!pip install scikit-learn
!pip install torch-geometric
!pip install implicit
!pip install torch-geometric
!pip install implicit

import pandas as pd

# Load the dataset
df = pd.read_csv("/content/dataset.csv", encoding='latin-1')

# Preview the data
print(df.columns)
print(df.head())

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler

# Load and select relevant features
df = pd.read_csv("/content/dataset.csv", encoding='latin-1')

# Select features and target
features = [
    'danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness',
    'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo'
]
X = df[features]
y = df['track_genre']

# Encode target
le = LabelEncoder()
y_encoded = le.fit_transform(y)

# Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)

# Scale features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

"""Train a Random classifier

"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

# Train model
clf = RandomForestClassifier(n_estimators=100, random_state=42)
clf.fit(X_train, y_train)

# Evaluate
y_pred = clf.predict(X_test)
print(classification_report(y_test, y_pred, target_names=le.classes_))

"""Train XGBoost for Better Accuracy

"""

import xgboost as xgb

xgb_clf = xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')
xgb_clf.fit(X_train, y_train)

y_pred_xgb = xgb_clf.predict(X_test)
print(classification_report(y_test, y_pred_xgb, target_names=le.classes_))

!pip install torch torchvision torchaudio
!pip install torch-geometric

import torch
from torch_geometric.data import Data
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.neighbors import kneighbors_graph
import pandas as pd
import numpy as np

# Load dataset
df = pd.read_csv("/content/dataset.csv", encoding='latin-1')

# Select numeric features
features = [
    'danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness',
    'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo'
]
X = df[features]
y = df['track_genre']

# Encode labels
le = LabelEncoder()
y_encoded = le.fit_transform(y)

# Scale features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Create k-NN graph (each node connected to k nearest neighbors)
k = 5
adj = kneighbors_graph(X_scaled, k, mode='connectivity', include_self=False)
edge_index = torch.tensor(np.array(adj.nonzero()), dtype=torch.long)

# Convert to torch tensors
x = torch.tensor(X_scaled, dtype=torch.float)
y = torch.tensor(y_encoded, dtype=torch.long)

# Create PyG graph object
data = Data(x=x, edge_index=edge_index, y=y)
print(data)

from torch.nn import Linear
import torch.nn.functional as F
from torch_geometric.nn import GCNConv
from torch_geometric.loader import DataLoader
from sklearn.model_selection import train_test_split

# Train/val split
num_nodes = data.num_nodes
train_mask = torch.zeros(num_nodes, dtype=torch.bool)
val_mask = torch.zeros(num_nodes, dtype=torch.bool)

train_ids, val_ids = train_test_split(range(num_nodes), test_size=0.2, random_state=42)
train_mask[train_ids] = True
val_mask[val_ids] = True

data.train_mask = train_mask
data.val_mask = val_mask

# Define the GCN model
class GCN(torch.nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels):
        super().__init__()
        self.conv1 = GCNConv(in_channels, hidden_channels)
        self.conv2 = GCNConv(hidden_channels, out_channels)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        return F.log_softmax(x, dim=1)

model = GCN(in_channels=x.shape[1], hidden_channels=32, out_channels=len(le.classes_))
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
criterion = torch.nn.CrossEntropyLoss()

for epoch in range(100):
    model.train()
    optimizer.zero_grad()
    out = model(data.x, data.edge_index)
    loss = criterion(out[data.train_mask], data.y[data.train_mask])
    loss.backward()
    optimizer.step()

    model.eval()
    _, pred = out[data.val_mask].max(dim=1)
    correct = pred.eq(data.y[data.val_mask]).sum().item()
    acc = correct / data.val_mask.sum().item()
    if epoch % 10 == 0:
        print(f"Epoch {epoch}, Loss: {loss.item():.4f}, Val Acc: {acc:.4f}")

model.eval()
out = model(data.x, data.edge_index)
_, pred = out.max(dim=1)
from sklearn.metrics import classification_report
print(classification_report(data.y[val_mask].cpu(), pred[val_mask].cpu(), target_names=le.classes_))

pip install lyricsgenius transformers

import lyricsgenius
genius = lyricsgenius.Genius("your_genius_api_key_here")

def get_lyrics(track, artist):
    try:
        song = genius.search_song(track, artist)
        return song.lyrics if song else None
    except:
        return None

# Add a lyrics column (sample first 100)
df_sample = df[['track_name', 'artists']].head(100).copy()
df_sample['lyrics'] = df_sample.apply(lambda x: get_lyrics(x['track_name'], x['artists']), axis=1)

# Step 1: Install required libraries
!pip install spotipy

# Step 2: Import and set credentials
import spotipy
from spotipy.oauth2 import SpotifyOAuth

# Step 3: Define your client credentials
CLIENT_ID = 'your_client_id_here'
CLIENT_SECRET = 'your_client_secret_here'
REDIRECT_URI = 'http://localhost:8888/callback'

# Step 4: Set scope (permission to access certain data)
scope = 'user-library-read'

# Step 5: Initialize Spotipy OAuth flow
sp_oauth = SpotifyOAuth(client_id=CLIENT_ID,
                        client_secret=CLIENT_SECRET,
                        redirect_uri=REDIRECT_URI,
                        scope=scope)

# Step 6: Get authorization URL
auth_url = sp_oauth.get_authorize_url()
print(f"Go to this URL and authorize the app:\n{auth_url}")

pip install transformers datasets spotipy

import lyricsgenius
genius = lyricsgenius.Genius("UbWe-mXzc3d2fkWPuiCB6D7rNZFbRY66gejTPBJbNkmUIYoXRNKoFY4QgR7q9h0T")

def get_lyrics(track, artist):
    try:
        song = genius.search_song(track, artist)
        return song.lyrics if song else None
    except:
        return None

# Add a lyrics column (sample first 100)
df_sample = df[['track_name', 'artists']].head(100).copy()
df_sample['lyrics'] = df_sample.apply(lambda x: get_lyrics(x['track_name'], x['artists']), axis=1)

print(df.columns.tolist())

import pandas as pd
import matplotlib.pyplot as plt

# Load CSV
df = pd.read_csv("/content/dataset.csv", encoding='latin-1')

# Strip any spaces in column names
df.columns = df.columns.str.strip()

# Check if 'track_genre' exists
if 'track_genre' in df.columns:
    # Show most common genres
    print("Top 10 genres:")
    print(df['track_genre'].value_counts().head(10))

    # Plot genre distribution
    plt.figure(figsize=(12,6))
    df['track_genre'].value_counts().head(10).plot(kind='bar', color='skyblue')
    plt.title('Top 10 Most Common Music Genres')
    plt.xlabel('Genre')
    plt.ylabel('Number of Tracks')
    plt.xticks(rotation=45)
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    plt.tight_layout()
    plt.show()
else:
    print("Error: 'track_genre' column not found in the dataset.")

!pip install transformers datasets scikit-learn

import pandas as pd

# Try specifying a different encoding
df = pd.read_csv("/content/dataset.csv", encoding="ISO-8859-1")
df.columns = df.columns.str.strip()

# Check the first few rows
print(df.head())

# Example text data for training and validation
train_texts = ["This is a training text", "Another training text"]
val_texts = ["This is a validation text", "Another validation text"]

# Tokenizer initialization
from transformers import AutoTokenizer

MODEL_NAME = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

# Encoding the text
train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=32)
val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=32)

# Check the tokenized outputs
print(train_encodings)
print(val_encodings)

import torch
from torch.utils.data import Dataset

# Example of dataset class
class MusicGenreDataset(Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        return {
            'input_ids': torch.tensor(self.encodings['input_ids'][idx]),
            'attention_mask': torch.tensor(self.encodings['attention_mask'][idx]),
        } | {'labels': torch.tensor(self.labels[idx])}

# Example text and labels for training and validation
train_texts = ["This is a training text", "Another training text"]
val_texts = ["This is a validation text", "Another validation text"]

train_labels = [0, 1]  # Example labels for the training set (e.g., music genre classes)
val_labels = [0, 1]    # Example labels for the validation set

# Tokenizer initialization
from transformers import AutoTokenizer

MODEL_NAME = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

# Encoding the text
train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=32)
val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=32)

# Encoding the labels (this step is typically necessary if you're working with categorical labels)
# Assuming train_labels and val_labels are already integers, but they can be transformed to tensors
train_labels_enc = torch.tensor(train_labels)
val_labels_enc = torch.tensor(val_labels)

# Create datasets
train_dataset = MusicGenreDataset(train_encodings, train_labels_enc)
val_dataset = MusicGenreDataset(val_encodings, val_labels_enc)

# Check the first item in the dataset
print(train_dataset[0])

from sklearn.preprocessing import LabelEncoder
from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments

# Example labels (typically these would come from your dataset)
train_labels = ["class1", "class2", "class1", "class3"]
val_labels = ["class2", "class3", "class1"]

# Step 1: Initialize LabelEncoder
label_enc = LabelEncoder()

# Step 2: Fit the encoder on the training labels (or both if needed)
label_enc.fit(train_labels)

# Step 3: Get the number of unique labels (num_labels)
num_labels = len(label_enc.classes_)

# Step 4: Initialize the model with the number of labels
MODEL_NAME = "bert-base-uncased"
model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)

# Example of how to use the encoder to transform labels
train_labels_enc = label_enc.transform(train_labels)
val_labels_enc = label_enc.transform(val_labels)

# Print number of labels and encoded labels
print("Number of labels:", num_labels)
print("Encoded train labels:", train_labels_enc)

# Assuming you have already defined `train_dataset` and `val_dataset` (as shown in earlier examples)
from torch.utils.data import DataLoader

# Create DataLoader instances for training and validation
train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)
val_dataloader = DataLoader(val_dataset, batch_size=16)

pip install --upgrade transformers

import transformers
print(transformers.__version__)

import transformers
print(transformers.__version__)

pip show transformers

from transformers import TrainingArguments

# Set training arguments
training_args = TrainingArguments(
    output_dir='./results',              # Output directory
    num_train_epochs=3,                  # Number of training epochs
    per_device_train_batch_size=16,      # Batch size for training
    per_device_eval_batch_size=16,       # Batch size for evaluation
    logging_dir='./logs',                # Directory for storing logs
    logging_steps=10                     # Log every 10 steps
)

print(training_args)

from transformers import TrainingArguments, TrainerState, TrainerControl, SchedulerType

training_args = TrainingArguments(
    output_dir='./results',              # Output directory
    num_train_epochs=3,                  # Number of training epochs
    per_device_train_batch_size=16,      # Batch size for training
    per_device_eval_batch_size=16,       # Batch size for evaluation
    logging_dir='./logs',                # Directory for storing logs
    logging_steps=10,                    # Log every 10 steps
    logging_strategy="steps",            # Use "steps" logging strategy
    lr_scheduler_type=SchedulerType.LINEAR, # Linear learning rate scheduler
    max_grad_norm=1.0,                   # Gradient clipping
    save_steps=500,                      # Save the model every 500 steps
    # save_strategy=SaveStrategy.STEPS,    # Save strategy based on steps - replaced with save_strategy
    save_strategy="steps",                # Save strategy based on steps
    overwrite_output_dir=False,          # Don't overwrite the output directory
    seed=42,                             # Random seed for reproducibility
    weight_decay=0.01,                   # Weight decay for regularization
    # optim=OptimizerNames.ADAMW_TORCH,    # Optimizer used for training - replaced with optim
    optim="adamw_torch",                  # Optimizer used for training
    report_to=["tensorboard", "wandb"],  # Reporting to TensorBoard and Weights & Biases
    push_to_hub=False,                   # Disable push to HuggingFace Hub (set to True if needed)
    use_cpu=False,                       # Set to True if you want to use the CPU
    warmup_steps=500,                    # Warmup steps for learning rate scheduler
    remove_unused_columns=True,          # Remove unused columns from the dataset
)

print(training_args)

from transformers import AutoModelForSequenceClassification

MODEL_NAME = "bert-base-uncased"  # You can use any pre-trained model

# Load the pre-trained model for sequence classification
model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=3)  # Adjust num_labels as per your task

from datasets import load_dataset

# Load a dataset (use your dataset here)
dataset = load_dataset("imdb")  # Example: IMDb dataset for sentiment analysis

# Tokenize the dataset
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

# Tokenize the training and validation texts
def tokenize_function(examples):
    return tokenizer(examples['text'], padding=True, truncation=True)

train_dataset = dataset['train'].map(tokenize_function, batched=True)
val_dataset = dataset['test'].map(tokenize_function, batched=True)

from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    save_steps=500,
    save_total_limit=2,
    logging_steps=10,
    logging_dir='./logs',
    report_to="none"  # Disable wandb if you don't want to use it
)

from transformers import Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset
)

from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    save_steps=500,
    save_total_limit=2,
    logging_steps=10,
    logging_dir='./logs',
    report_to="none"  # Disable wandb if you don't want to use it
)

import pandas as pd
from sklearn.model_selection import train_test_split

# Load the dataframe
df = pd.read_csv("/content/dataset.csv", encoding='latin-1')  # Or the correct path and encoding

# Check the actual column names in your dataframe
print(df.columns)

# Replace 'genres' with the actual name of the column containing the genre labels
# For example, if the column is named 'track_genre', use:
train_df, val_df = train_test_split(df, test_size=0.2, stratify=df["track_genre"], random_state=42)

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# Split the dataset
train_df, val_df = train_test_split(df, test_size=0.2, stratify=df["track_genre"], random_state=42)

# Extract labels
train_labels = train_df["track_genre"]
val_labels = val_df["track_genre"]

# Encode labels
label_enc = LabelEncoder()
train_labels_enc = label_enc.fit_transform(train_labels)
val_labels_enc = label_enc.transform(val_labels)

# Get number of unique classes
num_labels = len(label_enc.classes_)

from transformers import AutoModelForSequenceClassification

MODEL_NAME = "bert-base-uncased"  # or your preferred model
model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)

from huggingface_hub import login
login(token="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")

from transformers import AutoTokenizer
from datasets import Dataset
import pandas as pd

# Load the tokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# Fill missing values in 'track_name' with an empty string or a placeholder
train_df['track_name'] = train_df['track_name'].fillna("unknown")
val_df['track_name'] = val_df['track_name'].fillna("unknown")

# Convert DataFrame column to list of strings
train_texts = train_df["track_name"].astype(str).tolist()
val_texts = val_df["track_name"].astype(str).tolist()

# Tokenize function that applies the tokenizer to each text individually
def tokenize_function(examples):
    return tokenizer(examples['text'], padding=True, truncation=True, max_length=128)

# Convert the dataframes to datasets
train_dataset = Dataset.from_dict({"text": train_texts, "label": train_df["track_genre"].tolist()})
val_dataset = Dataset.from_dict({"text": val_texts, "label": val_df["track_genre"].tolist()})

# Apply the tokenizer to the datasets
train_dataset = train_dataset.map(tokenize_function, batched=True)
val_dataset = val_dataset.map(tokenize_function, batched=True)

# Check the results
print(train_dataset[0])

from transformers import AutoModelForSequenceClassification

# Load a pre-trained model with the number of labels for classification
model = AutoModelForSequenceClassification.from_pretrained(
    "bert-base-uncased", num_labels=len(train_df["track_genre"].unique())
)

# Convert the datasets into format suitable for the model
train_dataset = train_dataset.rename_column("label", "labels")
val_dataset = val_dataset.rename_column("label", "labels")

# Set the format of the dataset for PyTorch
train_dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "labels"])
val_dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "labels"])

pip install torch-geometric

import torch
from torch_geometric.data import Data

# Example: Define the number of nodes and features
num_nodes = 100  # For example, 100 nodes (texts, words, or documents)
num_features = 128  # For example, embedding size of 128 (could be the size of word embeddings)

# Now create the feature matrix (nodes x features)
x = torch.rand((num_nodes, num_features))  # Randomly initialized feature matrix for example

# Define some simple edges between nodes (this could be based on your graph structure)
edge_index = torch.tensor([
    [0, 1, 2],  # source node indices
    [1, 2, 0]   # target node indices
], dtype=torch.long)  # Example graph edges

# Create PyTorch Geometric Data object
data = Data(x=x, edge_index=edge_index)

import torch
import torch.nn.functional as F
from torch_geometric.nn import GCNConv

class GCNModel(torch.nn.Module):
    def __init__(self, num_features, num_classes):
        super(GCNModel, self).__init__()
        # GCN layers
        self.conv1 = GCNConv(num_features, 16)
        self.conv2 = GCNConv(16, num_classes)

    def forward(self, data):
        x, edge_index = data.x, data.edge_index

        # First GCN layer with ReLU activation
        x = F.relu(self.conv1(x, edge_index))

        # Second GCN layer
        x = self.conv2(x, edge_index)

        # Apply softmax on output
        return F.log_softmax(x, dim=1)

# Example: Initialize model
model = GCNModel(num_features=128, num_classes=10)  # Modify num_features and num_classes

import torch
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv
from torch_geometric.loader import DataLoader

# Assuming num_nodes, num_features are already defined

# Example features
num_nodes = 100
num_features = 128
x = torch.rand((num_nodes, num_features))  # Node features

# Example edge_index (graph structure)
edge_index = torch.tensor([
    [0, 1, 2],
    [1, 2, 0]
], dtype=torch.long)

# Example labels (for graph classification, you need to have target labels for each graph)
# These should be provided based on your task
labels = torch.randint(0, 2, (num_nodes,))  # Example binary classification labels for nodes

# Create Data object with features, edge_index, and labels
data = Data(x=x, edge_index=edge_index, y=labels)

# Define a simple GCN model
class GCN(torch.nn.Module):
    def __init__(self, in_channels, out_channels):
        super(GCN, self).__init__()
        self.conv1 = GCNConv(in_channels, 64)
        self.conv2 = GCNConv(64, out_channels)

    def forward(self, data):
        x, edge_index = data.x, data.edge_index
        x = F.relu(self.conv1(x, edge_index))
        x = F.dropout(x, training=self.training)
        x = self.conv2(x, edge_index)
        return F.log_softmax(x, dim=1)

# Initialize the model
model = GCN(in_channels=num_features, out_channels=2)  # Assuming binary classification (2 classes)

# Define the optimizer
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# Training loop
model.train()
optimizer.zero_grad()

# Forward pass
out = model(data)  # Assuming `data` has labels (`y`) attached

# Calculate loss
loss = F.nll_loss(out, data.y)  # `data.y` is the target labels

# Backward pass and optimization
loss.backward()
optimizer.step()

# Print the loss
print(f"Loss: {loss.item()}")

import torch.optim as optim

# Create a DataLoader (if needed)
from torch_geometric.loader import DataLoader

# Example training loop
optimizer = optim.Adam(model.parameters(), lr=0.01)

# Assume you have a DataLoader for your dataset (in this case, just a single graph for simplicity)
loader = DataLoader([data], batch_size=1)

model.train()
for epoch in range(10):
    for batch in loader:
        optimizer.zero_grad()
        out = model(batch)  # Forward pass
        loss = F.nll_loss(out, batch.y)  # Assuming you have labels `y` in your data object
        loss.backward()
        optimizer.step()

    print(f'Epoch {epoch}, Loss: {loss.item()}')

model.eval()
with torch.no_grad():
    correct = 0
    total = 0
    for batch in loader:
        out = model(batch)
        pred = out.argmax(dim=1)
        correct += (pred == batch.y).sum().item()
        total += batch.num_graphs

    print(f'Accuracy: {correct / total:.4f}')

import torch
import torch.nn.functional as F
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv
from torch_geometric.loader import DataLoader

# Example data preparation (same as above)
num_nodes = 100
num_features = 128
x = torch.rand((num_nodes, num_features))  # Node features
edge_index = torch.tensor([
    [0, 1, 2],
    [1, 2, 0]
], dtype=torch.long)
labels = torch.randint(0, 2, (num_nodes,))  # Example binary classification labels

# Create Data object
data = Data(x=x, edge_index=edge_index, y=labels)

# Define the GCN model (same as above)
class GCN(torch.nn.Module):
    def __init__(self, in_channels, out_channels):
        super(GCN, self).__init__()
        self.conv1 = GCNConv(in_channels, 64)
        self.conv2 = GCNConv(64, out_channels)

    def forward(self, data):
        x, edge_index = data.x, data.edge_index
        x = F.relu(self.conv1(x, edge_index))
        x = F.dropout(x, training=self.training)
        x = self.conv2(x, edge_index)
        return F.log_softmax(x, dim=1)

# Initialize model and optimizer
model = GCN(in_channels=num_features, out_channels=2)
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# Training loop (simplified)
model.train()
optimizer.zero_grad()
out = model(data)  # Forward pass
loss = F.nll_loss(out, data.y)
loss.backward()
optimizer.step()

# Get node embeddings after training (using the output of the second GCN layer)
embeddings = model.conv2(model.conv1(data.x, data.edge_index), data.edge_index)

# Reduce dimensionality for visualization (t-SNE)
tsne = TSNE(n_components=2, random_state=42)
embeddings_2d = tsne.fit_transform(embeddings.detach().numpy())

# Plotting the embeddings
plt.figure(figsize=(8, 6))
scatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=data.y, cmap='jet')
plt.title('Node Embeddings Visualization (t-SNE)')
plt.xlabel('t-SNE Component 1')
plt.ylabel('t-SNE Component 2')
plt.colorbar(scatter)
plt.show()

# Get model predictions
model.eval()
with torch.no_grad():
    pred = model(data).max(dim=1)[1]  # Predicted class labels

# Compare predictions to true labels
plt.figure(figsize=(8, 6))
scatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=pred, cmap='jet')
plt.title('Model Predictions vs. True Labels')
plt.xlabel('t-SNE Component 1')
plt.ylabel('t-SNE Component 2')
plt.colorbar(scatter)
plt.show()

# Example: Plot training loss over epochs
epochs = 100
losses = []

for epoch in range(epochs):
    model.train()
    optimizer.zero_grad()
    out = model(data)  # Forward pass
    loss = F.nll_loss(out, data.y)
    loss.backward()
    optimizer.step()

    losses.append(loss.item())

# Plot loss
plt.plot(range(epochs), losses)
plt.title('Training Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.show()

import pandas as pd

# Load the dataset
df = pd.read_csv("/content/dataset.csv", encoding='latin-1')

# Display the current columns
print("Columns:", df.columns.tolist())
df.head()

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

# Load your dataset
df = pd.read_csv("/content/dataset.csv", encoding='latin-1')

# Drop rows with missing values in relevant columns
df = df.dropna(subset=['track_genre', 'popularity'])

# Use these features for modeling
features = [
    'danceability', 'energy', 'loudness', 'speechiness', 'acousticness',
    'instrumentalness', 'liveness', 'valence', 'tempo', 'duration_ms',
    'popularity',  # used as rating
    'explicit'     # boolean feature
]

# Convert 'explicit' to numeric (True â†’ 1, False â†’ 0)
df['explicit'] = df['explicit'].astype(int)

X = df[features]
y = df['track_genre']

# Encode the genre (target)
le = LabelEncoder()
y_encoded = le.fit_transform(y)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)

# Feature scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train classifier
clf = RandomForestClassifier(n_estimators=100, random_state=42)
clf.fit(X_train_scaled, y_train)

# Evaluate
y_pred = clf.predict(X_test_scaled)
print(classification_report(y_test, y_pred, target_names=le.classes_))

import numpy as np

num_users = 500  # Number of synthetic users
user_ids = [f"user_{i}" for i in range(num_users)]

# Randomly assign songs to users as "liked"
df_users = pd.DataFrame({
    "user_id": np.random.choice(user_ids, size=len(df)),
    "track_id": df['track_id'],
    "liked": np.random.choice([0, 1], size=len(df), p=[0.7, 0.3])  # 30% liked, 70% not liked
})

# Merge with original dataset
df_full = df.merge(df_users, on="track_id")

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Add boolean 'explicit' as int
df_full['explicit'] = df_full['explicit'].astype(int)

# Feature set
features = [
    'danceability', 'energy', 'loudness', 'speechiness', 'acousticness',
    'instrumentalness', 'liveness', 'valence', 'tempo', 'duration_ms',
    'popularity', 'explicit'
]

X = df_full[features]
y = df_full['liked']  # Whether the user liked the song

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

clf = RandomForestClassifier(n_estimators=100, random_state=42)
clf.fit(X_train_scaled, y_train)

y_pred = clf.predict(X_test_scaled)
print(classification_report(y_test, y_pred))

def recommend_songs(user_id, model, df_full, features, scaler, top_n=10):
    user_data = df_full[df_full['user_id'] == user_id]
    all_songs = df_full.drop_duplicates(subset=['track_id'])

    # Predict songs the user hasn't seen
    unseen = all_songs[~all_songs['track_id'].isin(user_data['track_id'])]
    X_unseen = scaler.transform(unseen[features])
    unseen['score'] = model.predict_proba(X_unseen)[:, 1]  # probability of 'liked'

    recommendations = unseen.sort_values(by='score', ascending=False).head(top_n)
    return recommendations[['track_name', 'artists', 'track_genre', 'score']]

# Example: Recommend songs for user_42
recommend_songs("user_42", clf, df_full, features, scaler, top_n=5)

from matplotlib import pyplot as plt
import seaborn as sns
_df_2.groupby('track_genre').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))
plt.gca().spines[['top', 'right',]].set_visible(False)

from matplotlib import pyplot as plt
import seaborn as sns
_df_2.groupby('track_genre').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))
plt.gca().spines[['top', 'right',]].set_visible(False)

pip install plotly

import plotly.express as px

# Get recommendations for a user
top_recs = recommend_songs("user_42", clf, df_full, features, scaler, top_n=10)

# Plot recommendation scores
fig = px.bar(top_recs,
             x='track_name',
             y='score',
             color='track_genre',
             hover_data=['artists'],
             title='Top 10 Recommended Songs for user_42',
             labels={'score': 'Predicted Like Score'})
fig.update_layout(xaxis_tickangle=-45)
fig.show()

pip install scikit-surprise

!pip install --upgrade scikit-surprise numpy

# Step 1: Upgrade numpy
!pip install --upgrade numpy

# Step 2: Uninstall surprise if already installed
!pip uninstall -y scikit-surprise

# Step 3: Reinstall surprise from a compatible source
!pip install scikit-surprise --no-binary :all:

def cf_recommend_songs(user_id, df_full, model_cf, top_n=10):
    user_data = df_full[df_full['user_id'] == user_id]
    all_songs = df_full['track_id'].unique()

    seen = set(user_data['track_id'])
    unseen = [iid for iid in all_songs if iid not in seen]

    predictions = [model_cf.predict(user_id, song_id) for song_id in unseen]
    sorted_preds = sorted(predictions, key=lambda x: x.est, reverse=True)[:top_n]

    track_ids = [pred.iid for pred in sorted_preds]
    top_tracks = df_full[df_full['track_id'].isin(track_ids)][['track_name', 'artists', 'track_genre']].drop_duplicates()
    top_tracks['score'] = [pred.est for pred in sorted_preds]

    return top_tracks

!pip install spotipy
!pip install nltk

import spotipy
from spotipy.oauth2 import SpotifyClientCredentials

client_id = "6d2b50260a9a484a87a2844dfd4fef16"
client_secret = "a4801d4d92fb4c008b4a5b1bfcfba2db"

auth_manager = SpotifyClientCredentials(client_id=client_id, client_secret=client_secret)
sp = spotipy.Spotify(auth_manager=auth_manager)

def spotify_chatbot(query):
    query = query.lower()

    if "artist" in query:
        # Extract artist name after the word "artist"
        artist_name = query.split("artist")[-1].strip()
        results = sp.search(q=f'artist:{artist_name}', type='artist', limit=1)
        items = results['artists']['items']
        if items:
            artist = items[0]
            return f"Artist: {artist['name']}\nFollowers: {artist['followers']['total']}\nGenres: {', '.join(artist['genres'])}"
        else:
            return "Sorry, I couldn't find that artist."

    elif "track" in query or "song" in query:
        # Extract track or song name
        if "track" in query:
            track_name = query.split("track")[-1].strip()
        else:
            track_name = query.split("song")[-1].strip()

        results = sp.search(q=f'track:{track_name}', type='track', limit=1)
        items = results['tracks']['items']
        if items:
            track = items[0]
            return f"Track: {track['name']} by {track['artists'][0]['name']}\nAlbum: {track['album']['name']}\nPopularity: {track['popularity']}"
        else:
            return "Sorry, I couldn't find that track."

    else:
        return "Please ask about an artist or a song!"

print(spotify_chatbot("Tell me about artist Adele"))
print(spotify_chatbot("Information on track Shape of You"))
print(spotify_chatbot("Give me details about the song Blinding Lights"))

!pip install ipywidgets
from IPython.display import display
import ipywidgets as widgets

chat_output = widgets.Output()

def on_submit(text):
    with chat_output:
        print(f"You: {text.value}")
        response = spotify_chatbot(text.value)
        print(f"Bot: {response}\n")
        text.value = ''

input_box = widgets.Text(placeholder='Ask about a song or artist...')
input_box.on_submit(on_submit)

display(input_box, chat_output)

!pip install gradio

import gradio as gr

def chatbot_fn(message):
    return spotify_chatbot(message)

iface = gr.Interface(fn=chatbot_fn, inputs="text", outputs="text", title="Spotify Chatbot")
iface.launch()

!pip install spotipy gradio

import spotipy
from spotipy.oauth2 import SpotifyClientCredentials

client_id = '6d2b50260a9a484a87a2844dfd4fef16'
client_secret = 'a4801d4d92fb4c008b4a5b1bfcfba2db'

auth_manager = SpotifyClientCredentials(client_id=client_id, client_secret=client_secret)
sp = spotipy.Spotify(auth_manager=auth_manager)

def spotify_chatbot(query):
    results = sp.search(q=query, type='track', limit=1)
    if results['tracks']['items']:
        track = results['tracks']['items'][0]
        name = track['name']
        artist = track['artists'][0]['name']
        album = track['album']['name']
        url = track['external_urls']['spotify']
        preview = track['preview_url']
        return f"ðŸŽµ *{name}* by *{artist}*\nAlbum: {album}\nListen: {url}\nPreview: {preview if preview else 'No preview available'}"
    else:
        return "Sorry, I couldn't find that song. Try another title!"

import gradio as gr

iface = gr.Interface(fn=spotify_chatbot, inputs="text", outputs="text", title="ðŸŽ§ Spotify Song Finder Chatbot")
iface.launch()

import pandas as pd

# Load the dataset (update the path if needed)
df = pd.read_csv("/content/dataset.csv", encoding='latin1')  # or 'utf-8' if applicable

# Clean column names
df.columns = df.columns.str.strip()

# Drop rows with any NaNs in selected columns
df = df.dropna(subset=features + ['track_genre']).reset_index(drop=True)

import pandas as pd
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import classification_report
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split

# Load data
df = pd.read_csv("/content/dataset.csv", encoding='latin1')
df.columns = df.columns.str.strip()

# Select features and drop NaNs
features = [
    'danceability', 'energy', 'key', 'loudness', 'mode',
    'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo'
]
df = df.dropna(subset=features + ['track_genre']).reset_index(drop=True)

# Encode target
le = LabelEncoder()
y = le.fit_transform(df['track_genre'])
X = df[features]

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Train Naive Bayes
nb = GaussianNB()
nb.fit(X_train, y_train)
y_pred_nb = nb.predict(X_test)

# Evaluation
print("Naive Bayes:\n", classification_report(y_test, y_pred_nb, target_names=le.classes_))

print(type(X))
print(X.shape)

from sklearn.decomposition import TruncatedSVD

n_components = min(50, X.shape[1])  # this will be 10 here

svd = TruncatedSVD(n_components=n_components)
X_reduced = svd.fit_transform(X)

from sklearn.decomposition import TruncatedSVD

n_components = min(50, X.shape[1])  # This will be 10 for your data

svd = TruncatedSVD(n_components=n_components)
X_reduced = svd.fit_transform(X)

# Then compute similarity matrix
from sklearn.metrics.pairwise import cosine_similarity
similarity_matrix = cosine_similarity(X_reduced)

from sklearn.decomposition import TruncatedSVD
from sklearn.metrics.pairwise import cosine_similarity

# Ensure n_components <= number of features
n_components = min(50, X.shape[1])  # Here, n_components = 10

# Initialize TruncatedSVD with correct number of components
svd = TruncatedSVD(n_components=n_components)

# Fit and transform the data
X_reduced = svd.fit_transform(X)

# Compute cosine similarity matrix on the reduced features
similarity_matrix = cosine_similarity(X_reduced)

# Example: print the shape of the similarity matrix
print("Similarity matrix shape:", similarity_matrix.shape)

# Optional: print similarity matrix or part of it
print(similarity_matrix[:5, :5])

import os
print(os.getcwd())

import os
print(os.listdir('/content'))

import pandas as pd

df = pd.read_csv('dataset.csv', encoding='latin1')
print(df.head())
print(df.columns)

# Select feature columns (excluding non-numeric or irrelevant columns)
feature_cols = ['danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness',
                'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo',
                'time_signature']

X = df[feature_cols]
y = df['track_genre']

print(X.shape, y.shape)

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

X = df[feature_cols]
y = df['track_genre']

# Encode target if needed (e.g., if it's categorical text)
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
y_encoded = le.fit_transform(y)

# Split
X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)

# Train model
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

# Predict and evaluate
y_pred = rf.predict(X_test)
print(classification_report(y_test, y_pred, target_names=le.classes_))

import numpy as np

# Find unique classes in y_test
unique_labels = np.unique(y_test)

# Generate classification report with correct labels and target names subset
print(classification_report(y_test, y_pred, labels=unique_labels, target_names=le.inverse_transform(unique_labels)))

!pip install librosa

import librosa
import os
import IPython.display as ipd

# Example: Load a sample audio from librosa
audio_path = librosa.example('trumpet')
y, sr = librosa.load(audio_path)

print(f"Audio duration: {len(y)/sr:.2f} seconds")
ipd.Audio(y, rate=sr)  # Play audio

import numpy as np
unique, counts = np.unique(y, return_counts=True)
print(dict(zip(le.classes_, counts)))

df = df.dropna(subset=[target_col])

y = le.fit_transform(df[target_col])

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.decomposition import TruncatedSVD
from sklearn.pipeline import Pipeline
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import classification_report, accuracy_score

# Load dataset
df = pd.read_csv('/content/dataset.csv', encoding='latin1')  # use 'ISO-8859-1' if needed

# Drop rows with missing genre labels
df = df.dropna(subset=['track_genre'])

# Define target
target_col = 'track_genre'

# Drop non-numeric, irrelevant columns
drop_cols = ['Unnamed: 0', 'track_id', 'artists', 'album_name', 'track_name', target_col]
X = df.drop(columns=drop_cols, errors='ignore')

# Select numeric features only
X = X.select_dtypes(include=['number'])

# Scale features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Encode labels
le = LabelEncoder()
y = le.fit_transform(df[target_col])

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42, stratify=y
)

# Define models
nb_model = GaussianNB()

svd_lr_model = Pipeline([
    ('svd', TruncatedSVD(n_components=min(10, X_train.shape[1]-1))),
    ('lr', LogisticRegression(max_iter=2000, solver='liblinear'))
])

rf_model = RandomForestClassifier(n_estimators=100, random_state=42)

svm_model = SVC(probability=True, random_state=42)

# Fusion model using soft voting
fusion_model = VotingClassifier(
    estimators=[
        ('rf', rf_model),
        ('svm', svm_model),
        ('lr', LogisticRegression(max_iter=2000, solver='liblinear'))
    ],
    voting='soft'
)

# Dictionary of models
models = {
    "Naive Bayes": nb_model,
    "SVD + Logistic Regression": svd_lr_model,
    "Random Forest": rf_model,
    "SVM": svm_model,
    "Fusion (Voting Classifier)": fusion_model
}

# Train and evaluate each model
for name, model in models.items():
    print(f"\nTraining and evaluating: {name}")
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    print(f"Accuracy: {acc:.4f}")
    print(classification_report(y_test, y_pred, target_names=le.classes_, zero_division=0))

# Concatenate audio features + lyrics embeddings
X_fused = pd.concat([X_audio_features, X_lyrics_embeddings], axis=1)

rf_fusion = RandomForestClassifier(n_estimators=100, random_state=42)
rf_fusion.fit(X_fused_train, y_train)
y_pred_fusion = rf_fusion.predict(X_fused_test)
print("Fusion Strategy (RF):\n", classification_report(y_test, y_pred_fusion, target_names=le.classes_))

import os
import librosa

folder_path = 'audio_files'
audio_features_list = []

for filename in os.listdir(folder_path):
    if filename.endswith('.wav'):  # or other extensions you support
        file_path = os.path.join(folder_path, filename)
        y, sr = librosa.load(file_path)
        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)
        mfccs_mean = mfccs.mean(axis=1)
        audio_features_list.append(mfccs_mean)

# Convert to DataFrame
import pandas as pd
X_audio_features = pd.DataFrame(audio_features_list, columns=[f'mfcc_{i}' for i in range(1, 14)])

import librosa
import numpy as np
import pandas as pd

# Example: loading an audio file
y, sr = librosa.load('your_audio_file.wav')

# Extract features (mean MFCCs over time)
mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)
mfccs_mean = np.mean(mfccs, axis=1)

# Convert to DataFrame (1 sample)
X_audio_features = pd.DataFrame([mfccs_mean], columns=[f'mfcc_{i}' for i in range(1, 14)])

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.metrics import classification_report, accuracy_score, adjusted_rand_score
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.decomposition import TruncatedSVD
from sklearn.pipeline import Pipeline
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import OneHotEncoder
import matplotlib.pyplot as plt

# Load dataset
df = pd.read_csv('/content/dataset.csv', encoding='latin1')

# Define selected features and target
feature_cols = ['danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness',
                'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo',
                'time_signature']
target_col = 'track_genre'

# Filter dataset
df = df.dropna(subset=feature_cols + [target_col])
X = df[feature_cols]
le = LabelEncoder()
y = le.fit_transform(df[target_col])

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42, test_size=0.2)

# ===========================
# 1. Traditional Models
# ===========================
models = {
    "Naive Bayes": GaussianNB(),
    "SVD + Logistic Regression": Pipeline([
        ('svd', TruncatedSVD(n_components=min(10, X_train.shape[1]-1))),
        ('lr', LogisticRegression(max_iter=1000))
    ]),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
    "SVM": SVC(probability=True, random_state=42),
    "Fusion (Voting)": VotingClassifier(
        estimators=[
            ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),
            ('svm', SVC(probability=True, random_state=42)),
            ('lr', LogisticRegression(max_iter=1000))
        ],
        voting='soft'
    )
}

model_scores = {}

for name, model in models.items():
    print(f"\nâœ… Training and evaluating: {name}")
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    print(f"Accuracy: {acc:.4f}")
    print(classification_report(y_test, y_pred, target_names=le.classes_))
    model_scores[name] = acc

# ===========================
# 2. UBSO â€“ Clustering-Based
# ===========================
print("\nâœ… UBSO (Clustering with KMeans)")
kmeans = KMeans(n_clusters=len(le.classes_), random_state=42)
clusters = kmeans.fit_predict(X)
ari_score = adjusted_rand_score(y, clusters)
print("Adjusted Rand Index (UBSO):", ari_score)
model_scores["UBSO"] = ari_score

# ===========================
# 3. MASO â€“ Matrix Attention
# ===========================
print("\nâœ… MASO-style Attention Prediction")

# One-hot encode the training labels
enc = OneHotEncoder(sparse_output=False)  # Use this instead of sparse=False
y_train_onehot = enc.fit_transform(y_train.reshape(-1, 1))

# Compute cosine similarity as attention weights
attention_weights = cosine_similarity(X_test, X_train)

# Weighted label outputs using attention
weighted_outputs = attention_weights @ y_train_onehot

# Predict label with highest weighted score
y_pred_maso = np.argmax(weighted_outputs, axis=1)
y_pred_maso_labels = enc.inverse_transform(weighted_outputs)

# Evaluation
from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred_maso_labels))

# ===========================
# 4. FLAN-T5 (Text Prompting)
# ===========================
print("\nâœ… FLAN-T5 Genre Prediction (Text-based)")
!pip install transformers -q
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline

tokenizer = AutoTokenizer.from_pretrained("google/flan-t5-base")
model = AutoModelForSeq2SeqLM.from_pretrained("google/flan-t5-base")
flan_pipe = pipeline("text2text-generation", model=model, tokenizer=tokenizer)

sample_texts = [f"What genre is the song '{t}' by '{a}'?" for t, a in zip(df['track_name'], df['artists'])]
preds = flan_pipe(sample_texts[:10])
print("Sample FLAN-T5 Outputs:")
for i, p in enumerate(preds):
    print(f"{sample_texts[i]} -> {p['generated_text']}")
model_scores["FLAN-T5"] = 0.40  # placeholder â€” manually evaluate or match with `y`

# ===========================
# 5. Gen S â€“ LLM Prompting (GPT-3)
# ===========================
print("\nâœ… Gen S â€“ LLM (GPT-3) Genre Prediction")
import openai
from openai import OpenAI

client = OpenAI(api_key="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")

response = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[{"role": "user", "content": "Predict the genre for: Track='To Begin Again', Artist='Ingrid Michaelson'"}],
    temperature=0.7
)

print("Genre prediction:", response.choices[0].message.content)

# ===========================
# Plot Comparison
# ===========================
results_df = pd.DataFrame({
    "Model": list(model_scores.keys()),
    "Score": list(model_scores.values())
})
results_df.sort_values("Score", ascending=False, inplace=True)

plt.figure(figsize=(10, 6))
plt.bar(results_df['Model'], results_df['Score'], color='skyblue')
plt.xticks(rotation=45, ha='right')
plt.title("Model Comparison (Accuracy or ARI)")
plt.ylabel("Score")
plt.grid(True)
plt.tight_layout()
plt.show()

print(df.columns)

!pip install --upgrade transformers datasets --quiet